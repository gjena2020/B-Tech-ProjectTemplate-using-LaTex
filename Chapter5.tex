\chapter{Simulation}

\section{Simulation on Open source self driving car simulator}

 While testing a trained CNN, we first evaluate the networks performance in simulation. A simplified block diagram of the simulation system is shown in Figure 1.2. The simulator~\cite{buyval2018realtime} takes pre-recorded frames of videos from a forward-facing on-board camera on a 3D Game-driven data-collection vehicle and generates images that approximate what would appear if the CNN were, instead, steering the vehicle. These test videos are time-synchronized with recorded steering commands generated by the player(training) driver.\\

Since the game drivers might not be driving in a particular lane all the time, we manually calibrate the lane associated with each frame in the video used by the simulator. We call this position the “ground truth”. The simulator transforms the original images to account for departures from the ground truth.\\

Note that this transformation also includes any discrepancy between the game driven path and the ground truth. The simulator accesses the recorded test video along with the synchronized steering commands that occurred when the video was captured. The simulator sends the first frame of the chosen test video, adjusted for any departures from the ground truth, to the input of the trained CNN. The CNN then returns a steering command for that frame. The CNN steering commands as well as the recorded training-driver commands are fed into the dynamic model [8] of the vehicle to update the position and orientation of the simulated vehicle. The simulator then modifies the next frame in the test video so that the image appears as if the vehicle were at the position that resulted by following steering commands from the CNN. This new image is then fed to the CNN and the process repeats. The simulator records the off-center distance (distance from the car to the lane center), the yaw, and the distance traveled by the virtual car. When the off-center distance exceeds one meter, a virtual training intervention is triggered, and the virtual vehicle position and orientation is reset to match the ground truth of the corresponding frame of the original test video. 

\subsection{Simulation process}
Once training is complete, we save the model to use it for driving the car autonomously in our simulator. We make a state dictionary and save the model with .h5 format using model.save(‘model.h5’).\\
Before we begin testing our model, we need a file that will load our model, get the frames of the track from the simulator to process through our model and send the steering prediction back to the simulator. We have made a drive.py file which basically a PyTorch version of Udacity’s drive.py which is a client-server program to establish a connection between our trained state dictionary model.h5 file and the simulation application to perform the autonomous driving of car in a completely new track.\\
	
The final step was to run the Udacity simulator in autonomous mode which used our CNN model. There were a few spots where the vehicle frequently fell off the track, especially during the turn after crossing the bridge. It sometimes ended in spectacular crashes or even in vehicle sinks. To improve the driving behavior in these cases, We tried to tune the model for better generalization, as discussed above. Sometimes we also added new samples covering just turns to emphasize these difficult situations in the dataset. But when it started working and the vehicle was able to drive autonomously around the track without leaving the road, our satisfaction was really big!\\
	
The server will be the simulator and the client would be the Neural Network, or rather the Python program. This whole process is a cyclic feedback loop. The simulator outputs the images, the python program analyses it and outputs the steering angle and the throttle. The simulator receives this and turns the car accordingly. And the whole process goes on cyclically.\\
	
Open the simulator again and now choose the autonomous mode. The car should drive on its own.\\\\
The CNN had never seen this track. The performance on the training track was a little off but fine as it shows that the car was not merely memorizing the track. It recovered successfully from a few critical situations, even though none of those maneuvers had been performed during training.\\\\
On the upper left corner a predicted steering angle is displayed.
	
\subsection{Conclusions}
Of course, a lot of upgrades can be done to this model. The main task in Behavioral Cloning project was to drive autonomously on the same track where the data was collected. But we had also an access to another, more challenging track - mountainous and with a lot of shadows on roads. . It would be closer to the real-world application requirements where vehicles need to handle new and very different situations during driving. We could further augment our data by introducing artificial shadows or by shifting the image in some directions. Also, better CNN architecture could be probably chosen, with more optimal parameters for this task.
	
\subsection{Possible improvements}
	
In 10 years most of us probably won’t own a car. We’ll have some subscription with some company like an Uber… and we would pay Rs. 10,000 approx. a month and every morning we wake up with a car in our driveway that will take us to work, if the law of the land~\cite{arentz2017driving} permits.\\
\newpage